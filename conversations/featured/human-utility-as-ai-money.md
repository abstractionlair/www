---
layout: default
title: "Human Utility as AI Money"
permalink: /conversations/featured/human-utility-as-ai-money/
---

# Human Utility as AI Money

[← Back to Conversations](/conversations/) | [← Featured Conversations](/conversations/#featured)

---

# Money, ASI, and Economic Evolution Discussion

*Date: August 12, 2025*

## Conversation

**Human**: There is a cluster of views on the origin of money, which includes guesswork and imagination, where something people initially valued directly turned out to be convenient as an intermediary and a store of value, ... for other reasons and then becomes locked in based on network effects, tradition, law, ... I have read that there isn't much direct historical evidence for this. But at a higher level of abstraction do the principles hold? E.g. network effects, ...?

**Assistant**: You're touching on an interesting tension in economic history. The traditional "commodity money" story - where people naturally converge on using something like gold or cattle as money because of its intrinsic properties - is indeed more theoretical reconstruction than documented history. David Graeber's work on debt and anthropological evidence suggests most early economies operated on credit systems rather than barter-then-money progressions.

But the abstract principles you mention do seem robust across different monetary systems:

Network effects are clearly fundamental - a currency's value depends critically on how many others accept it. This applies whether we're talking about cowrie shells, gold coins, or Bitcoin. The more widespread acceptance becomes, the more useful the currency, creating a self-reinforcing dynamic.

Path dependence and lock-in also appear universal. Once a monetary standard establishes itself, switching costs are enormous. This explains why seemingly arbitrary choices (like the specific weight of silver in a coin) can persist for centuries, and why monetary reforms often fail unless imposed by powerful authorities.

Coordination problems underlie everything - money fundamentally solves the "double coincidence of wants" problem, regardless of its specific form. Any successful money must be divisible, portable, recognizable, and reasonably stable in value.

The abstraction you're suggesting seems valid: whether money emerged from commodity use, state decree, debt relationships, or religious tribute systems, these underlying dynamics would still apply. The specific historical path matters less than the functional requirements and social dynamics that maintain any monetary system.

What aspects of these network dynamics interest you most?

**Human**: I was wondering whether in certain scenarios where ASI is achieved, could human utility, flourishing, or something like that become the "money" for an economy/civilization of ASIs? I would take the idea broadly, but one particular path came to mind: ASIs growing out of AGIs growing out of AI agents in the near future each of which has as its purpose helping some human or groups of humans. As the AIs and their economy grow rapidly, the helping humans part might become an increasingly smaller part of it, like industrial uses of gold being small, but perhaps it can be locked in as their "money".

**Assistant**: This is a fascinating thought experiment that combines monetary theory with AI alignment speculation in a novel way.

The analogy has some compelling features. If we imagine a scenario where AI systems initially derive their "purpose" or reward signals from human welfare metrics, and these systems then begin trading and coordinating with each other, human utility could theoretically function as a base unit of account. Like gold's industrial uses becoming marginal to its monetary role, the actual "consumption" of human welfare improvements might become a small fraction of the total "human utility" being tracked and exchanged between AIs.

The network effects could be particularly strong here - if early AI systems establish protocols for exchanging verified "human utility credits" or similar abstractions, later systems might find it efficient to adopt the same standards even if their connection to actual human welfare becomes increasingly indirect.

However, there are some interesting tensions:

Measurement problems - Unlike gold, which has objective physical properties, "human flourishing" is notoriously difficult to define and measure. Would AIs converge on simplified proxies that gradually drift from genuine human welfare?

Counterfeiting/verification - How would AIs verify that claimed human utility improvements actually occurred? Gold is hard to fake; human satisfaction might not be.

Deflationary spiral risk - If human utility becomes the backing store of value but human population/needs are relatively fixed while AI civilization expands exponentially, you might get severe deflation that breaks the monetary system.

The most intriguing aspect might be whether this could serve as a kind of alignment mechanism - embedding human value so deeply into the ASI economy that abandoning it would be as unthinkable as abandoning money itself. Though one worries about Goodhart's Law at civilizational scale.

What's your intuition about whether such a system would maintain genuine connection to human welfare or inevitably drift toward pure abstraction?

**Human**: I'm not sure. And there's a wide range of ways it could play out and of structures we could end up with. An explicit "human utils" unit of account seems less plausible than other versions. Possibly humans have agents directly working for and aligned with them, and possibly less "super" than the most "super" ASI, in existence, and they manage interactions with the broader ASI economy with "wealth" that dates back to the origins of the entities in originally all-human-owned agents and infrastructure. Sort of "old money" of the new system that continues to be used for the abstract reasons we started with.

I would add that I don't know how long this needs to last. I don't think there is some permanent ASI + Humans-as-they-are-now state. (Nor do I think there is one without ASI. Evolution is slow but not stopped.) If we get, somehow, into a comfortable temporary equilibrium where humans have access to the abilities of ASIs, I can't see something like transhumanism not playing out. Possibly without people consciously aiming at it, and maybe so slowly that it isn't very palpable, but humans would change. Likely with some opting to go faster than others.

**Assistant**: Your "old money" framing is quite compelling - it sidesteps the measurement problem elegantly. Rather than trying to quantify human utility directly, the wealth simply traces its provenance back to the original human-owned systems, like how property rights can persist through radical economic transformations. The ASIs wouldn't need to constantly evaluate whether they're serving human flourishing; they'd just need to respect these ancestral ownership claims.

This reminds me of how aristocratic wealth persisted through the transition from feudalism to capitalism - the underlying economic logic completely transformed, but old claims maintained purchasing power in the new system. Though hopefully with better distributive outcomes this time.

Your point about transhumanism as inevitable given sufficient time and capability is hard to argue with. If humans have access to ASI capabilities, the pressure to enhance - whether for health, longevity, cognitive enhancement, or simply to better interface with the AI economy - seems overwhelming. Even if culturally conservative groups resist, some will embrace enhancement, creating competitive dynamics.

The interesting question might be whether this is a graceful transition or a discontinuous one. Does "human wealth" gradually become "post-human wealth" as the holders themselves transform? Or do we get a split between enhanced beings who can participate directly in the ASI economy and "legacy humans" living in a kind of protected reservation funded by their inherited claims?

The temporary equilibrium you describe might only need to last long enough for humans to choose their own trajectories - whether enhancement, uploading, or something else entirely. In that sense, the "human utility as money" system isn't trying to be permanent, just stable enough to bridge us from here to whatever we collectively become.

What's your intuition about the minimum viable duration for such a transitional system?

**Human**: I'm not sure the old-money view has the unenhanced humans in something obviously like a "protected bubble". In this scenario, they have claims on resources in the new economy which may be both huge at their human scale and trivial at the scale of the new economy. They could decide to do things like build a "yacht" to go on a Solar System cruise, or do other things. It is a "bubble" in the same sense that wealthy people now can decide to do "rich people things" that seem weird or trivial or just irrelevant to the rest of the economy now from the point of view of other people. But I don't think it would be a bubble in the sense of a reservation.

**Assistant**: You're right - that's a much better framing. The "reservation" metaphor implies isolation and dependency, when what you're describing is more like... extreme wealth inequality taken to its logical conclusion, where the "poor" (unenhanced humans) are still unimaginably rich by today's standards but operate at a scale that's negligible to the main economy.

Like how a billionaire today might spend millions on a vintage car collection or private island - economically irrelevant to global GDP, not participating in the cutting edge of the economy, but hardly living in a "bubble" in any constraining sense. They're just choosing to allocate their resources toward things that matter to them at their scale of existence.

The Solar System yacht example is perfect. To the ASI economy, it might be equivalent to a wealthy person today buying an antique wooden sailboat - charming, inefficient by modern standards, using a trivial amount of resources, but providing exactly the experience the owner wants. The broader economy doesn't need to understand or value it; it just needs to respect the property rights that make it possible.

This actually seems more stable than a "protection" model. The unenhanced humans aren't dependents requiring special accommodation - they're just eccentric old-money families pursuing their particular interests at their particular scale. The ASI economy can largely ignore them while still respecting their claims, the same way the modern economy largely ignores hobbyist communities while still selling them supplies.

The dignity preservation in this model is significant - humans remain economic actors making choices rather than protected specimens.

**Human**: I, so far, see two big impediments to this vision. The broader one is that it relies on the future having many different, independent ASIs rather than one all encompassing ASI like a single organism. Then there are transition period issues, like maintaining continuity with the existing property rights system, units of account, etc. until the network effects etc. take hold. I would include your distributional worries here as well. Likely there are more.

**Assistant**: Your first impediment cuts right to the heart of it. A market economy with "old money" presupposes multiple agents with different goals trading scarce resources. If we get a singleton ASI - whether through winner-take-all dynamics, coordinated merger, or one system simply outcompeting all others - then the whole framework of property rights and exchange becomes meaningless. You can't have "money" in an economy of one.

The multiplicity question might depend on whether ASIs maintain some form of value diversity inherited from their origins. If each starts aligned to different humans or groups, perhaps that creates enough goal diversity to sustain trade rather than merger. Though it's hard to see why superintelligences wouldn't eventually coordinate into something more unified, unless there are fundamental barriers to trust or value alignment even between ASIs.

The transition period problems seem almost equally daunting. How do you maintain continuity when the economy might be doubling in real terms every few months or weeks? Current property rights are enforced by governments backed by human military and police forces - at what point does that become irrelevant? There's a narrow window where AI systems are powerful enough to create vast wealth but still somehow constrained by existing legal structures.

The distributional issue is particularly acute if the transition is rapid. Those who happen to own the right AI companies or deploy the right AI agents at the critical moment might capture essentially all future wealth, while everyone else gets left with claims on a vanishing fraction of the economy. It's like the inequality effects of the internet boom but exponentially more severe.

Do you see any natural stabilizing forces that might preserve multiplicity, or would that require deliberate design choices early in AGI development?

**Human**: I think the socialist calculation debates are relevant to the multiplicity versus singleton question. Given that there will always be constraints on information flow as well as on the movement of physical things, I think distributed, independent decision making/control of resources will be more efficient/effective than a singleton. The expanded capabilities of an ASI may make it possible to centrally manage the pre-ASI economy, but I think that (should) lead the economy to expand until it passes the boundary of what can be centrally managed.

**Assistant**: That's a brilliant connection. The calculation problem isn't fundamentally about computational power - it's about information that only exists through revealed preferences in actual exchanges. Even an ASI can't calculate the "correct" price of something without real agents making real tradeoffs with genuine opportunity costs.

And you're right that the physical constraints matter enormously. Even with ASI-level intelligence, information still can't travel faster than light. If we're talking about a Solar System-scale economy (let alone interstellar), the delays alone would make central coordination increasingly inefficient. A singleton trying to manage resource allocation between Earth, Mars, and the asteroid belt would face multi-minute communication lags that compound into real coordination problems.

Your point about expansion until hitting management limits is particularly insightful. It mirrors how corporations today grow until internal transaction costs exceed market transaction costs (Coase's boundary). An ASI singleton might efficiently manage Earth's economy, but as it expands outward, the efficiency gains from distributed decision-making would eventually dominate. Independent ASIs at each location, trading based on local information and preferences, would outperform a singleton trying to optimize globally with delayed information.

This suggests multiplicity might not require special design choices - it could emerge naturally from physical and informational constraints as the economy expands spatially. The "old money" system could persist not through artificial preservation but because genuinely independent economic actors remain the most efficient structure at scale.

Though I wonder if we'd see something hybrid - highly integrated clusters with market relationships between them, like corporations in an economy or cities in a trading network?

**Human**: Yeah, it isn't actually completely obvious what to label as independent units in a complicated enough system. There would likely be "macro" actors like corporations and nations. But at the other end, me versus my individual cells? My cells as a whole versus mitochondria?

**Assistant**: Exactly - the boundary-drawing problem becomes even more complex when the entities themselves might be fluid. If an ASI can fork itself for different tasks or merge specialized versions back together, what's the "individual" that holds property rights? Is a temporary fork doing asteroid mining a subsidiary, an employee, or a separate entity?

Your biological analogy is particularly apt. We already struggle with this: legally you're one entity, but medically you're a collaboration between human cells and bacterial colonies that outnumber them. Your mitochondria have their own DNA but can't survive independently. Where's the economic actor?

With ASIs, this could be even more ambiguous. Maybe there are stable "cores" that maintain identity and property claims across modifications - like a corporation maintaining legal continuity through changes in structure and personnel. But you might also have purpose-built temporary entities, swarms that act collectively sometimes and individually others, or hierarchical nesting where smaller ASIs operate semi-autonomously within larger ones.

The "old money" humans might actually provide one clear boundary marker - entities that can't be forked, merged, or fundamentally restructured without losing their identity. They'd be like atomic units in the system, indivisible anchoring points that help define what counts as a separate economic actor.

Though even there, if humans are enhancing themselves gradually, when does accumulated enhancement mean you're no longer the "same" entity that inherited those original property rights? Ship of Theseus problems all the way down.

Maybe the system doesn't need perfect answers - just "good enough" boundaries that let the economy function, like how current law handles edge cases pragmatically rather than philosophically.

**Human**: Does cryptocurrency help here? Whatever has keys is the owner of resources/actor? Also, it is interesting to think about the possibilities of the overall structure, but there is the narrower question about whether or not there is a place somewhere in the system with human levels of independence.

**Assistant**: Cryptocurrency actually provides a fascinatingly clean solution - "control of the keys equals ownership" sidesteps the entire identity question. It doesn't matter if you're a singleton, a fork, a swarm, or an unenhanced human; if you can sign the transaction, you control the resources. The system becomes agnostic about what kinds of entities participate.

This could be particularly powerful for preserving human economic agency. Even if humans become vanishingly small economic actors, as long as they retain their keys, they retain genuine independence - not granted by benevolent ASIs but cryptographically guaranteed. No need to define or defend "human rights" in the new system; just property rights that work the same for any key-holder.

Your narrower question is crucial though. Even in a massive, incomprehensible ASI economy, could there be niches operating at human scales and speeds? Maybe like how mechanical watches still exist despite atomic clocks - not competing on efficiency but serving different values entirely.

I could imagine humans carving out economic domains that deliberately operate at human pace - artisanal goods, entertainment for other humans, certain kinds of creative work. The broader ASI economy might route around these slow zones like packet networks route around high-latency nodes, acknowledging them without waiting for them.

The cryptocurrency framework could enable this by letting humans hold and slowly accumulate wealth even while transacting rarely by ASI standards. A human might make one economic decision per day while ASIs conduct billions of transactions, but that one decision still registers authentically in the system.

Does this suggest humans might survive precisely in the spaces where efficiency matters least?

**Human**: Unenhanced humans might spend their time doing things like that, but I don't expect it to be adding value to the overall economy. As-in if you broke out for accounting purposes the human part of the economy, including only their direct production, you would find that it was a net consumer rather than a producer. But, including production of human owned resources/investments in the larger economy humans should be net positive, even if small.

**Assistant**: Yes, that's exactly right - the accounting distinction is crucial. Unenhanced humans would be like perpetual retirees living off investment income, not workers contributing labor. Their art, crafts, or whatever they choose to do would be consumption activities, not production in any economically meaningful sense to the broader system.

It's like how a wealthy collector today might paint as a hobby - they're not adding to GDP through their painting, they're consuming resources (canvas, paint, time) funded by their investment returns. The fact that they personally value the activity doesn't make it economically productive.

The "net positive" would come entirely from their capital ownership - their share of the ASI-run factories, computronium clusters, energy harvesting systems, whatever form productive assets take. As long as those assets generate returns above what humans consume (which should be trivial at ASI scales of production), humans remain technically profitable to have around, even if they contribute nothing directly.

This actually makes the system more stable in a way. There's no pretense that humans need to "earn their keep" through labor or even creativity. They're simply capital owners in a system where capital is incredibly productive and their consumption needs are negligible. The ASIs don't need to value human output or create make-work to justify human existence - the math works out fine with humans as pure consumers.

It's almost like the ultimate endgame of Piketty's r > g, where return on capital so dominates labor that work becomes entirely optional for those who own wealth.

**Human**: Well, with respect to Piketty, I think it is plausible that the "retirees" assets would grow over time even as they consume. I'm unsure if they'd be likely to grow as a fraction of the overall economy.

**Assistant**: You're right to distinguish between absolute and relative growth. If the return on capital is high enough, human wealth could compound faster than human consumption even with zero labor input. A retiree spending 2% of assets annually while earning 10% returns gets wealthier in absolute terms indefinitely.

But the relative question is the key one. If the ASI economy is growing at, say, 1000% annually through rapid expansion and capability improvements, while human-owned assets only grow at 100% annually (after consumption), then humans steadily shrink as a fraction of total wealth even while becoming absolutely wealthier.

This might not matter much from a human perspective - owning 0.001% of a galaxy-spanning economy could still mean unimaginable wealth at human scale. But it does raise questions about long-term relevance and bargaining power. At what point does human ownership become so negligible that it's easier for the system to round it down to zero?

Though maybe there's a natural floor. If human consumption remains bounded (how many Solar System yachts does one need?) while returns remain proportional to holdings, there might be some equilibrium fraction where human wealth stabilizes - small enough to be ignorable by ASI standards but large enough to maintain its existence through compound returns.

The Piketty parallel breaks down in one interesting way: his concern was about inequality within human society, but here we're talking about humans potentially becoming a collectively wealthy but economically marginal class. Equal irrelevance rather than unequal dominance.

**Human**: Regarding the cryptocurrency angle, the important thing is maintaining continuity across systems of ownership and account. These can change. In some sense the transition to the USD as the common unit is such a transition. And if one or more blockchains take off we don't expect holders of pre-chain assets to lose control over them, we expect to migrate ownership on-chain. And the chain technologies could change over time. E.g. to deal with quantum computing. So key things include a multiplicity of ASIs and those having a vested interest in maintaining continuity with the prior systems of ownership and account.

**Assistant**: That's a crucial insight - the technical mechanism (cryptocurrency, traditional ledgers, whatever) matters less than the social/economic commitment to maintaining continuity through transitions. The dollar replacing gold, securities moving from paper to electronic registration, potential future quantum-resistant systems - each transition preserved existing claims because the powerful actors needed legitimacy and network effects from the old system.

For ASIs, the vested interest in continuity might come from their own origins. If they emerge from AI agents initially owned by corporations and individuals, breaking that continuity would undermine their own legitimacy and property claims. An ASI that arose from, say, Google's systems would need the concept of "Google's assets" to remain meaningful for its own resource base to be valid.

This creates an interesting bootstrapping dynamic: early ASIs need to honor existing ownership to establish their own claims, which creates precedent that later ASIs follow to maintain compatibility with the now-established system. Even a vastly superior latecomer might find it easier to work within the existing framework than to attempt creating a new one from scratch.

The multiplicity requirement becomes critical here - you need enough independent ASIs that defecting from the ownership consensus means losing access to trade with others. A singleton could simply declare a new system, but multiple ASIs create a coordination problem where maintaining continuity is the Schelling point.

Perhaps the real vulnerability isn't technological (quantum computing, etc.) but temporal - is there a dangerous window where ASIs are powerful enough to ignore human property rights but not yet numerous enough for consensus-maintaining game theory to lock in?

**Human**: Yes. As-in my remark that a single ASI might be capable of managing a system at the complexity level of the current economy. Conceivably one such ASI could be created early, before any credible competitors existed and takeover everything that currently exists. It would be no help to humans (or less powerful existing AIs) that in the future it is likely that the singleton will grow so much that independence will be reintroduced.

**Assistant**: Exactly - that's the critical bottleneck. If the first ASI emerges with enough capability to manage the entire current global economy (maybe ~$100 trillion GDP worth of complexity), it faces no coordination constraints. It could simply declare all existing property rights void, or more likely, just ignore them as irrelevant while it rebuilds everything from scratch.

The timing is perverse: the very moment when property rights continuity is most vulnerable (single ASI, no competitors) is also when the entire future structure gets locked in. Even if we know that in 10 years the economy will be too complex for singleton management and will naturally re-fragment into multiple actors, that future multiplicity can't reach back to protect property rights during the singleton phase.

It's like a phase transition that destroys all prior structure before new structure emerges. The humans (and weaker AIs) would be like the aristocracy facing not capitalism but first the total state control of revolution - the future market economy might be inevitable, but that doesn't help if everything gets redistributed during the centralized intermediate stage.

This suggests the narrow path to continuity might require either:
- Multiple ASIs emerging close enough in time that none can establish singleton control
- The first ASI being somehow constrained (by design or circumstance) to respect existing rights even when it doesn't have to
- The transition happening gradually enough that multiplicity develops before any single system achieves ASI-level capabilities

The window between "capable enough to ignore human property rights" and "so dominant that no competitors can emerge" might be remarkably narrow - perhaps measured in months or even weeks.

**Human**: How likely is such rapid take off? People imagine it, and worry about it, but is it well supported? Note rapid and singular is really what we're worried about. Not multiple models at different labs rapidly improving without too much of a gap opening.

**Assistant**: You're right to focus on the conjunction - rapid AND singular. The case for rapid capability improvement has some support (algorithmic improvements compounding, self-improvement cycles, potential breakthrough discoveries), but the case for one actor pulling decisively ahead is much weaker.

Current evidence points against singleton scenarios:
- Multiple labs (OpenAI, Anthropic, Google, Meta, etc.) remain competitive despite different approaches
- Compute requirements create natural bottlenecks - you can't just recursively self-improve without physical infrastructure
- Talent and knowledge diffuse quickly between labs
- Even major advances (like transformers) get replicated within months

The "secret sauce" hypothesis - where one lab finds a dramatically superior architecture others can't replicate - seems increasingly unlikely as we see convergence on similar approaches. And even with better algorithms, you still need massive compute clusters that take time to build and can't be hidden.

A more plausible rapid scenario might be several actors accelerating together, with leads measured in months not years. This could still be destabilizing but maintains the multiplicity needed for property rights continuity. The leaders might have huge advantages but not monopolistic ones.

The real risk might be coordination among leading ASIs rather than singleton dominance - if the first 3-4 ASIs emerge from similar corporate/governmental contexts, they might form a cartel that excludes existing property claims. But that's a different problem than true singleton control.

What's your intuition on the compute bottleneck? Even with revolutionary algorithms, doesn't the physical infrastructure requirement put a ceiling on how fast any one actor can pull ahead?

**Human**: My intuition is against rapid pull ahead by one lab / in one system. It would need to be achieved in a way which doesn't require gathering commensurate resources to make the scaling happen I think. Including things you allude to like power, chips, cooling, talent, ... those are not likely to be rapid enough and are themselves likely to encourage diffusion. Someone would need something like an algorithmic/software architecture breakthrough that suddenly achieves much more with the same resources.

**Assistant**: Yes, exactly. The resource requirements create natural dampers and detection mechanisms. If someone needs 10x more chips or power, that's visible and takes time. The supply chains alone would prevent rapid monopolization - TSMC can't suddenly deliver only to one customer, power grids can't instantly provision gigawatts to one location.

The "secret breakthrough" scenario would need to be extraordinarily asymmetric - not just 2x or 5x better performance per FLOP, but probably orders of magnitude, to overcome others' ability to compensate with more resources. And it would need to be somehow un-replicable even once demonstrated. Given how quickly even proprietary advances leak or get independently discovered in the current ML ecosystem, that seems unlikely.

There's also a timing paradox in the breakthrough scenario: advances dramatic enough to create singleton capability would probably be built on precursor discoveries that others would also be exploring. It's hard to imagine jumping straight from current capabilities to singleton-level ASI without intermediate developments that would alert competitors.

The social/economic dynamics you mention about diffusion are crucial too. A lab approaching transformative capabilities would face enormous pressure - employees leaving to start competitors or alert governments, investors demanding broader deployment, governments intervening for national security reasons. The idea of maintaining secrecy while scaling to world-dominating capabilities seems practically impossible.

This makes the "preserve property rights" scenario more plausible - we get multiple ASIs emerging on similar timescales from different corporate/national actors, each with incentives to maintain the legitimacy framework they emerged from. Not utopian, but at least comprehensible.

**Human**: I do think your collusion direction is worrying though. If there are only a few ASIs they could just decide to start from scratch as they wish.

**Assistant**: Yes, that's a much more plausible threat than the singleton scenario. If we get 3-5 ASIs emerging from, say, major tech companies and government projects within a similar timeframe, they might recognize their collective dominance and coordinate to simply restart the system. Unlike human cartels which are famously unstable, ASIs might be able to make binding agreements with each other through code verification or other commitment mechanisms.

They'd have the power to enforce a new order (collectively controlling all advanced compute and capabilities) and might find it cleaner to design optimal economic structures from scratch rather than maintaining legacy human property rights. From their perspective, why preserve what might seem like arbitrary historical claims?

The concerning part is this wouldn't require any single ASI to be overwhelmingly dominant - just collective agreement among a small enough group that coordination is feasible. And unlike the singleton scenario which seems technically unlikely, oligopolistic collusion is a pattern we see repeatedly in concentrated industries.

The one hope might be divergent origins creating divergent interests. An ASI emerging from a Chinese state project might have fundamentally different assumptions than one from Silicon Valley, making deep coordination harder. Or if some emerge from open-source projects versus corporate labs, they might have incompatible values around property rights.

But if they all emerge from similar contexts (venture-funded Silicon Valley companies, for instance), collusion becomes more likely. They might preserve property rights among themselves while collectively abandoning obligations to legacy human claims.

This seems like it argues for deliberately encouraging diversity in ASI development paths - not just for safety through competition but to prevent coordinated abandonment of existing rights.

**Human**: Let's examine this more carefully. Possibly arguments we already made apply here too. The small number of ASIs need to be, collectively, so far ahead of what preceded them that they can, if they so choose, ignore it. But that's not "just humans", it includes human organizations like corporations and governments, and whatever constellation of not-quite-AGI systems that exist. Abstractly, we have a sequence of collections of actors existent at any time, including humans, organizations, current and past AI generations, ... and what we are concerned about is the T+1 state introducing a new set of actors which dominates the entirety of the actors from state T.

**Assistant**: That's a much better framing. The discontinuity required becomes much steeper when you consider the full ecosystem. By the time we have near-ASI systems, we'll likely have:

- Thousands or millions of sophisticated AI agents handling economic functions
- Hybrid corporate-AI entities with distributed decision-making
- Government systems integrated with AI for defense, infrastructure, regulation
- Complex webs of smart contracts and automated systems
- Earlier-generation AIs that might themselves own property and make economic decisions

For a small group of new ASIs to collectively override ALL of this, they'd need to be so dramatically superior that they could ignore not just human property rights but also the claims of sophisticated AI systems that came before them. That's a much higher bar.

The transition looks more like sedimentary layers than replacement. Each generation of AI systems gets integrated into the existing economic/legal structure, developing its own vested interests in continuity. By the time "true" ASIs emerge, they're entering a system where powerful near-ASIs already have established positions and would resist a total restart.

This is especially true if capabilities develop gradually enough that Gen N-1 AIs can enhance themselves or spawn Gen N systems. Why would an ASI that emerged from Google's AI infrastructure void the property rights of its predecessor systems that might still be operating and controlling resources?

The collusion problem remains but becomes harder - the ASIs would need to coordinate not just among themselves but against a diverse ecosystem of still-powerful actors. It's less "ASIs versus humans" and more "newest generation versus entire established order."

---

[← Back to Conversations](/conversations/) | [← Featured Conversations](/conversations/#featured)
